% This document is based on http://math.shinshu-u.ac.jp/~hanaki/beamer/beamer.html
\documentclass[dvipdfmx,cjk]{beamer}
%\documentclass[dvipdfm,cjk]{beamer} % オプションは環境や利用するプログラムによって変える
%\documentclass[dvips,cjk]{beamer}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{latexsym}


% しおり（PDF にしたときの目次）の文字化け防止
\AtBeginDvi{\special{pdf:tounicode 90ms-RKSJ-UCS2}}
%\AtBeginDvi{\special{pdf:tounicode EUC-UCS2}}

% 右下のアイコンを消す
\setbeamertemplate{navigation symbols}{}

% テーマ
\usetheme{CambridgeUS}
%\usetheme{Boadilla}           %% Beamer のディレクトリの中の
%\usetheme{Madrid}             %% beamerthemeCambridgeUS.sty を指定
%\usetheme{Antibes}            %% 色々と試してみるといいだろう
%\usetheme{Montpellier}        %% サンプルが beamer\doc に色々とある。
%\usetheme{Berkeley}
%\usetheme{Goettingen}
%\usetheme{Singapore}
%\usetheme{Szeged}

\usecolortheme{rose}          %% colortheme を選ぶと色使いが変わる
%\usecolortheme{albatross}

%\useoutertheme{shadow}                 %% 箱に影をつける
\usefonttheme{professionalfonts}       %% 数式の文字を通常の LaTeX と同じにする

%\setbeamercovered{transparent}         %% 消えている文字をうっすらと表示する
%\setbeamertemplate{theorems}[numbered]  %% 定理に番号をつける
\newtheorem{thm}{Theorem}[section]
\newtheorem{proposition}[thm]{Proposition}
\theoremstyle{example}
\newtheorem{exam}[thm]{Example}
\newtheorem{remark}[thm]{Remark}
\newtheorem{question}[thm]{Question}
\newtheorem{prob}[thm]{Problem}
\newtheorem{rev}[thm]{Review}
\DeclareMathOperator{\argmin}{argmin}

% メタ情報
\begin{document}
\title[]{Data Science and Machine Learning}
\author[]{照屋 佑喜仁}
\institute[]{}
\date{\today}

% タイトルスライド
\begin{frame}
    \titlepage
\end{frame}

% 目次（\section 名が自動で挿入される）
\begin{frame}
    \tableofcontents
\end{frame}

\section{2.4 Tradeoffs in Statical Learning}
\subsection{教師あり学習の技術}
\begin{frame}
    \frametitle{教師あり学習}
    \begin{itemize}
        \item 教師あり学習における機械学習の技術
              \begin{itemize}
                  \item generalization risk(2.5)あるいはexpected generalization risk(2.6)をできるだけ小さくする
                  \item できるだけ少ない計算リソースで
              \end{itemize}
        \item これを達成するために，適切な予測関数の集合$\mathcal{G}$を選ぶ必要がある．この選び方は下のような要因によって決まる．
              \begin{itemize}
                  \item 集合の複雑さ(最適な予測関数$g^*$を適切に近似，あるいは含むのに十分に複雑(豊か)か？)
                  \item (2.4)の最適化によって学習者を訓練する容易さ
                  \item 集合$\mathcal{G}$において，training loss(2.3)がrisk(2.1)をどれだけ正確に推定するか
                  \item 連続なのか，分類なのか……
              \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Tradeoff}
\begin{frame}
    \frametitle{Tradeoff}
    \begin{itemize}
        \item 集合$\mathcal{G}$の選択は，通常トレードオフを伴う
              \begin{itemize}
                  \item 単純な$\mathcal{G}$からの学習器は早く訓練できるが，上手く近似できない可能性
                  \item $g^*$を含むような豊かな$\mathcal{G}$からの学習器は多くの計算リソースを必要とする可能性
              \end{itemize}
        \item モデルの複雑さ，計算の単純さ，推定の制度の関係を見るために2つのtradeoffについて考えていく
              \begin{itemize}
                  \item the approxiation-estimation tradeoff(近似-推定トレードオフ)
                  \item the bias-variance tradeoff(バイアス-分散トレードオフ)
              \end{itemize}
    \end{itemize}
    今，generalization risk(2.5)を３つの要素に分解して考える．
    \begin{align*}
        \ell(g^\mathcal{G}_\tau)=\underbrace{\ell^*}_\text{irreducible risk}+\underbrace{\ell(g^\mathcal{G})-\ell^*}_\text{approximation error}+\underbrace{\ell(g^\mathcal{G}_\tau)-\ell(g^\mathcal{G})}_\text{statistical error} \tag{2.16}
    \end{align*}
\end{frame}
\subsection{irreducible risk, approxiation error, statistical error}
\begin{frame}
    \frametitle{irreducible risk，approximation error}
    \begin{align*}
        \ell(g^\mathcal{G}_\tau)=\underbrace{\ell^*}_\text{irreducible risk}+\underbrace{\ell(g^\mathcal{G})-\ell^*}_\text{approximation error}+\underbrace{\ell(g^\mathcal{G}_\tau)-\ell(g^\mathcal{G})}_\text{statistical error}\tag{2.16}
    \end{align*}
    \begin{itemize}
        \item \alert{$\ell^*$}は$\ell(g^*)$で定義されるirreducible risk(還元不能リスク)．どの学習器も$\ell^*$より小さいリスクで予測することはできない．
        \item \alert{$g^\mathcal{G}$}は$\argmin_{g\in\mathcal{G}}\ell(g)$で定義される，$\mathcal{G}$内で最も最良の学習器．
        \item \alert{$\ell(g^\mathcal{G})-\ell^*$}はapproximation error(近似誤差)．irreducible riskと$\mathcal{G}$のなかで最良の予測関数のriskの差を見ている．
              \begin{itemize}
                  \item 適切な$\mathcal{G}$を選び, その上で$\ell(g)$を最小化するのは，単純に数値解析と関数解析の問題となる(ここで訓練データ$\tau$は登場しないから)
                  \item $\mathcal{G}$が$g^*$を含まなければ近似誤差は任意に小さく出来ずriskを大きくする要因となる
                  \item 近似誤差を減らす唯一の方法は，$\mathcal{G}$を大きくしてより多くの関数を含めること
              \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{statical (estimation) error}
    \begin{align*}
        \ell(g^\mathcal{G}_\tau)=\underbrace{\ell^*}_\text{irreducible risk}+\underbrace{\ell(g^\mathcal{G})-\ell^*}_\text{approximation error}+\underbrace{\ell(g^\mathcal{G}_\tau)-\ell(g^\mathcal{G})}_\text{statistical error}\tag{2.16}
    \end{align*}
    \begin{itemize}
        \item \alert{$\ell(g^\mathcal{G}_\tau)-\ell(g^\mathcal{G})$}はstatistical(estimation) error(統計的(推定)誤差)．訓練セット$\tau$に依存．特に，学習器$g^\mathcal{G}_\tau$が$\mathcal{G}$の最良の予測関数$g^\mathcal{G}$をどれだけ上手く推定しているかに依存している．\\(良い予測器なら)この誤差は訓練サイズが無限大に近づくにつれて(確率的に，または期待値として)0に収束するはずである．
    \end{itemize}
\end{frame}
\subsection{approximation-estimation tradeoff}
\begin{frame}
    \frametitle{approximation-estimation tradeoff}
    approximation-estimation tradeoff(近似-推定トレードオフ)は，２つの相反する要求を対立させる．
    \begin{itemize}
        \item $\mathcal{G}$が十分にシンプルで，統計的誤差が大きくなりすぎない必要がある．（推定しやすい？）
        \item $\mathcal{G}$が十分に充実して，近似誤差が小さいことを保証する必要がある．（$g^*$をできれば見つけたい？）
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{2乗誤差損失でのリスクを解釈してみる}
    2乗誤差損失のリスクは$\ell(g^\mathcal{G}_\tau)=\mathbb{E}\left[(Y-g^\mathcal{G}_\tau(\boldsymbol{X}))^2\right]$となる．このとき，最適な予測関数は$g^*(\boldsymbol{x})=\mathbb{E}\left[Y\mid \boldsymbol{X}=\boldsymbol{x}\right]$で与えられるのであった．(theorem2.1)\\
    このとき，分解(2.16)は以下のように解釈できる．
    \begin{itemize}
        \item $\ell^*=\mathbb{E}\left[(Y-g^*(\boldsymbol{X}))^2\right]$は還元不能誤差であり，これより小さい期待2乗誤差の予測関数はない．
        \item 近似誤差$\ell(g^\mathcal{G})-\ell(g^*)$は$\mathbb{E}\left[g^\mathcal{G}(\boldsymbol{X})-g^*(\boldsymbol{X})^2\right]$に等しい(excrcise2)．つまり，最適予測値と$\mathcal{G}$内の最適予測値との間の２乗誤差の期待値として解釈できる．
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{２乗誤差損失でのリスクを解釈してみる}
    \begin{itemize}
        \item 統計的誤差$\ell(g^\mathcal{G}_\tau)-\ell(g^\mathcal{G})$に関しては$\mathcal{G}$が線形関数の集合である場合を除いて，期待２乗誤差(mean squared error 平均2乗誤差ともいう)としての直接的な解釈は存在しない．\\
              $\mathcal{G}$が線形関数集合の場合，関数$g(\boldsymbol{x})$はあるベクトル$\boldsymbol{\beta}$に対して$g(\boldsymbol{x})=\boldsymbol{x}^T\boldsymbol{\beta}$と表すことができ，統計的誤差は$\mathbb{E}\left[(g^\mathcal{G}_\tau(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X}))^2\right]$となる(excercise3)．
    \end{itemize}
    以上より，２乗誤差損失を用いる場合，線形関数集合$\mathcal{G}$に対するgeneralization riskは
    \begin{align*}
        \ell(g^\mathcal{G}_\tau) & =\mathbb{E}\left[(g^\mathcal{G}_\tau(\boldsymbol{X})-Y)^2\right] \\&=\ell^*+\underbrace{\mathbb{E}\left[g^\mathcal{G}(\boldsymbol{X})-g^*(\boldsymbol{X})^2\right]}_\text{近似誤差}+\underbrace{\mathbb{E}\left[(g^\mathcal{G}_\tau(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X}))^2\right]}_\text{統計的誤差}
    \end{align*}
    統計的誤差だけが訓練データに依存する唯一の項であることに注意．
\end{frame}

\subsection{excercise2}
\begin{frame}
    \frametitle{excercise2}
    保留していた証明を行う．まず，
    \begin{align*}
        \ell(g^\mathcal{G})-\ell(g^*)=\mathbb{E}\left[(g^\mathcal{G}(\boldsymbol{X})-g^*(\boldsymbol{X}))^2\right]
    \end{align*}
    を示す．
    \begin{proof}
        \begin{align*}
            \ell(g^\mathcal{G}) & =\mathbb{E}\left[(Y-g^\mathcal{G}(\boldsymbol{X}))^2\right] \quad \text{(2乗誤差を採用したときの定義)}                                                                                                   \\
                                & =\mathbb{E}\left[\left\{Y-g^*(\boldsymbol{X})+g^*(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X})\right\}^2\right]                                                                            \\
                                & =\underbrace{\mathbb{E}\left[\left\{Y-g^*(\boldsymbol{X})\right\}^2\right]}_\text{$\ell(g^*)$の定義}+\mathbb{E}\left[\left\{g^*(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X})\right\}^2\right] \\
                                & \qquad-\underbrace{2\mathbb{E}\left[\left\{Y-g^*(\boldsymbol{X})\right\}\left\{g^*(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X})\right\}\right]}_\text{$\bigstar$}
        \end{align*}
        \renewcommand{\qedsymbol}{}
    \end{proof}
\end{frame}

\begin{frame}
    \frametitle{excercise2}
    \begin{proof}
        ここで，前々回のTheorem2.1の証明と同様の議論により$\bigstar=0$となるので，
        \begin{align*}
            \ell(g^\mathcal{G})-\ell(g^*)=\mathbb{E}\left[(g^\mathcal{G}(\boldsymbol{X})-g^*(\boldsymbol{X}))^2\right]
        \end{align*}
        を得る．
    \end{proof}
    \begin{remark}[tower property, taking out what is known]
        tower property:
        \begin{align*}
            \mathbb{E}[\mathbb{E}[Y|X]] = \mathbb{E}[Y]
        \end{align*}
        taking out what is known:
        \begin{align*}
            \mathbb{E}[XY|X]=X\mathbb{E}[Y|X]
        \end{align*}
    \end{remark}
\end{frame}
\begin{frame}
    \frametitle{参考(前々回のスライド)}
    \begin{proof}
        定義より，
        \begin{align*}
            \mathbb{E}[Y-g^*(\boldsymbol{X})|\boldsymbol{X}] & =\int_\mathbb{R}(y-g^*(\boldsymbol{x}))f_{Y|\boldsymbol{X}}(y|\boldsymbol{x})dy                                                      \\
                                                             & =\int_\mathbb{R}yf_{Y|\boldsymbol{X}}(y|\boldsymbol{x})dy-\int_\mathbb{R}g^*(\boldsymbol{x})f_{Y|\boldsymbol{X}}(y|\boldsymbol{x})dy \\
                                                             & = \mathbb{E}[Y|\boldsymbol{X}]-\mathbb{E}[Y|X]\int_\mathbb{R}\frac{f(x,y)}{f_X(x)}dy                                                 \\
                                                             & =0                                                                                                                                   \\
            \because\int_\mathbb{R}\frac{f(x,y)}{f_X(x)}dy   & =\frac{1}{f_X(x)}\int_\mathbb{R}f(x,y)dy                                                                                             \\
                                                             & =\frac{1}{f_X(x)}f_X(x)=1
        \end{align*}
        \renewcommand{\qedsymbol}{}
    \end{proof}
\end{frame}

\subsection{exercise3}
\begin{frame}
    \frametitle{excercise3}
    次に以下を示す．
    \begin{align*}
        \ell(g^\mathcal{G}_\tau)-\ell(g^\mathcal{G})=\mathbb{E}\left[(g^\mathcal{G}_\tau(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X}))^2\right]
    \end{align*}
    \begin{proof}
        \begin{align*}
            \ell(g^\mathcal{G}_\tau) & =\mathbb{E}\left[\left\{Y-g^\mathcal{G}_\tau(\boldsymbol{X})\right\}^2\right]                                                             \\
                                     & =\mathbb{E}\left[\left\{Y-g^*(\boldsymbol{X})+g^*(\boldsymbol{X})-g^\mathcal{G}_\tau(\boldsymbol{X})\right\}^2\right]                     \\
                                     & =\ell(g^*)+\underbrace{\mathbb{E}\left[\left\{g^*(\boldsymbol{X})-g^\mathcal{G}_\tau(\boldsymbol{X})\right\}^2\right]}_\text{$\clubsuit$}
        \end{align*}
        \renewcommand{\qedsymbol}{}
    \end{proof}
\end{frame}

\begin{frame}
    \frametitle{excercise3}
    \begin{proof}
        ここで，
        \begin{align*}
            \clubsuit  & = \mathbb{E}\left[\left\{g^*(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X})+g^\mathcal{G}(\boldsymbol{X})-g^\mathcal{G}_\tau(\boldsymbol{X})\right\}^2\right]                                                     \\
                       & =\mathbb{E}\left[\left\{g^*(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X})\right\}^2\right]+\mathbb{E}\left[\left\{g^\mathcal{G}(\boldsymbol{X})-g^\mathcal{G}_\tau(\boldsymbol{X})\right\}^2\right]              \\
                       & \qquad -\underbrace{2\mathbb{E}\left[\left\{g^*(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X})\right\}\left\{g^\mathcal{G}(\boldsymbol{X})-g^\mathcal{G}_\tau(\boldsymbol{X})\right\}\right]}_\text{$\spadesuit$} \\
            \spadesuit & =2\mathbb{E}\left[\left\{g^*(\boldsymbol{X})-\boldsymbol{X}^T\boldsymbol{\beta}^\mathcal{G}\right\}\left\{\boldsymbol{X}^T\boldsymbol{\beta}^\mathcal{G}-\boldsymbol{X}^T\hat{\boldsymbol{\beta}}\right\}\right] \\
                       & =2\mathbb{E}\left[\left\{g^*(\boldsymbol{X})-\boldsymbol{X}^T\boldsymbol{\beta}^\mathcal{G}\right\}\boldsymbol{X}^T\right](\boldsymbol{\beta}^\mathcal{G}-\hat{\boldsymbol{\beta}})
        \end{align*}
        \renewcommand{\qedsymbol}{}
    \end{proof}
\end{frame}

\begin{frame}
    \frametitle{excercise3}
    \begin{proof}
        \begin{align*}
            \boldsymbol{\beta}^\mathcal{G}=\argmin_{\boldsymbol{\beta}}\mathbb{E}\left[\left\{g^*(\boldsymbol{X})-\boldsymbol{\beta}^T\boldsymbol{X}\right\}^2\right]
        \end{align*}
        であるため，$\frac{\partial}{\partial\boldsymbol{x}}g(f(\boldsymbol{x}))=\frac{\partial f}{\partial \boldsymbol{x}}\frac{\partial g}{\partial\boldsymbol{y}}$や微分と期待値の交換(中間値の定理，優収束定理などで示せる)などに気をつければ，
        \begin{align*}
                            & \left.\frac{\partial}{\partial\boldsymbol{\beta}}\mathbb{E}\left[\left\{g^*(\boldsymbol{X})-\boldsymbol{\beta}^T\boldsymbol{X}\right\}^2\right]\right|_{\boldsymbol{\beta}=\boldsymbol{\beta}^\mathcal{G}}=\boldsymbol{0} \\
            \therefore\quad & 2\mathbb{E}\left[X\left\{\boldsymbol{X}^T\boldsymbol{\beta}^\mathcal{G}\right\}\right]=\boldsymbol{0}
        \end{align*}
        よって$\spadesuit=0$となり，exercise2の結果と合わせて$\ell(g^\mathcal{G}_\tau)-\ell(g^\mathcal{G})=\mathbb{E}\left[(g^\mathcal{G}_\tau(\boldsymbol{X})-g^\mathcal{G}(\boldsymbol{X}))^2\right]$が成立．
    \end{proof}

\end{frame}

% ...前略...

\section{2.5 Bias-Variance Tradeoff}

\subsection{多項式回帰の例（続き）}
\begin{frame}
    \frametitle{Example 2.2 多項式回帰の例}
    $\mathcal{G} = \mathcal{G}_p$は$x$の線形関数の集合で，$\boldsymbol{x} \in [1, u, u^2, ..., u^{p-1}]^T$であり，$g^*(\boldsymbol{x}) = \boldsymbol{x}^T\boldsymbol{\beta}^*$である．

    また，$\boldsymbol{X}=\boldsymbol{x}$で，$Y = g^*(X) + \varepsilon(\boldsymbol{x})$，$\varepsilon(\boldsymbol{x}) \sim \mathcal{N}(0, \ell^*)$，$\ell^* = \mathbb{E}\left[\left\{Y-g^*(\boldsymbol{X})\right\}\right]$とする．

    まず，近似誤差を考える．任意の関数$g \in \mathcal{G}_p$は

    \begin{align*}
        g(x) = h(u) = \beta_1 + \beta_2 u + \cdots + \beta_p u^{p-1} = [1, u, ..., u^{p-1}]^T \boldsymbol{\beta}
    \end{align*}

    このとき$g(X)$は$[1, U, ..., U^{p-1}]\boldsymbol{\beta}$として分布し，$U \sim \mathcal{U}(0, 1)$であるとする．同様に，$g^*(\boldsymbol{X})$は$[1, U, U^2,U^3,U^4]\boldsymbol{\beta}^*$として分布するとする．
\end{frame}

\begin{frame}
    \frametitle{近似誤差の計算}
    近似誤差は
    \begin{align*}
        \int_{0}^{1}\left([1, U, ..., U^{p-1}]\boldsymbol{\beta}-[1, U, U^2,U^3,U^4]\boldsymbol{\beta}^*\right)^2 du
    \end{align*}
    となる．
    近似誤差を最小化するため，$\boldsymbol{\beta}$で微分して$0$になれば良い．ベクトルをベクトルで微分しており，$\boldsymbol{\beta}$の1つの要素に着目して微分すれば
    $$\int_0^{1} ([1, u, ..., u^{p-1}] \boldsymbol{\beta} - [1, u, u^2, u^3] \boldsymbol{\beta}^*) du = 0,$$
    $$\int_0^{1} ([1, u, ..., u^{p-1}] \boldsymbol{\beta} - [1, u, u^2, u^3] \boldsymbol{\beta}^*) u du = 0,$$
    $$\vdots$$
    $$\int_0^{1} ([1, u, ..., u^{p-1}]\boldsymbol{\beta} - [1, u, u^2, u^3] \boldsymbol{\beta}^*) u^{p-1} du = 0.$$


\end{frame}

\begin{frame}
    \frametitle{近似誤差の具体的な値}
    $\mathbf{H}_p = \int_0^{\infty} [1, u, ..., u^{p-1}]^T [1, u, ..., u^{p-1}] du$を$p \times p$ヒルベルト行列とする．

    $(i,j)$要素は$\int_0^{\infty} u^{i+j-2} du = 1/(i+j-1)$で与えられる．
    線形方程式系は$\mathbf{H}_p \boldsymbol{\beta} = \tilde{\mathbf{H}}\boldsymbol{\beta}^*$とまとめて表すことができる．ここで$\tilde{\mathbf{H}}$は$\mathbf{H}_p$の左上$(p \times 4)$ブロック．解は以下で表される：

    \[
        \boldsymbol{\beta}_p =
        \left\{
        \begin{array}{ll}
            \dfrac{65}{6}                                                                      & (p = 1)    \\[2ex]
            \begin{bmatrix} -\dfrac{20}{3}  & 35 \end{bmatrix}^{\mathrm{T}}                    & (p = 2)    \\[3ex]
            \begin{bmatrix} -\dfrac{5}{2} & 10 & 25 \end{bmatrix}^{\mathrm{T}}                 & (p = 3)    \\[3ex]
            \begin{bmatrix} 10 & -140 & 400 & -250 & 0 & \cdots & 0 \end{bmatrix}^{\mathrm{T}} & (p \geq 4)
        \end{array}
        \right.
        \tag{2.18}
    \]
\end{frame}

\begin{frame}
    \frametitle{近似誤差の結果}
    したがって，近似誤差$\mathbb{E}[(g^{\mathcal{G}_p}(X) - g^*(X))^2]$は以下で与えられる：

    \[
        \int_0^{1} ([1, u, ..., u^{p-1}]\boldsymbol{\beta}_p - [1, u, u^2, u^3]\boldsymbol{\beta}^*)^2 du \simeq
        \begin{cases}
            127.9, & p = 1, \\
            25.8,  & p = 2, \\
            22.3,  & p = 3, \\
            0,     & p > 4.
        \end{cases}
        \tag{2.19}
    \]

    $p$が増加するにつれて近似誤差が小さくなることに注目．この例では，$p = 4$で近似誤差がゼロとなる（$p > 4$も同様）．

    一般に，近似する関数のクラス$\mathcal{G}$がより複雑になると，近似誤差は減少する．(前に述べた)
\end{frame}


\begin{frame}
    \frametitle{統計的誤差}
    次に，統計的誤差の典型的な挙動を説明する．2乗誤差損失に対して，統計的誤差は以下のように書ける：

    \begin{align*}
        \int_0^{1} ([1, ..., u^{p-1}](\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_p))^2du= (\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_p)^T\mathbf{H}_p (\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_p) \quad \tag{2.20}
    \end{align*}

    図2.8は，図2.7でテスト損失の計算に使用されたものと同じ訓練セットに対する凡化リスクの分解(2.17)を示している．

    テストロスは独立したテストデータを使用して推定されることに注意．この場合，2つがよく一致していることがわかる．統計的誤差の最小値は$p = 4$付近にあることがわかる．
\end{frame}

\begin{frame}
    \frametitle{図2.8の解釈}
    \begin{center}
        % 図2.8の代替テキスト表現
        \begin{tabular}{|c|c|}
            \hline
            パラメータ数$p$ & リスクの構成要素     \\
            \hline
            低い$p$     & 近似誤差大，統計的誤差小 \\
            高い$p$     & 近似誤差小，統計的誤差大 \\
            \hline
        \end{tabular}
    \end{center}

    \begin{itemize}
        \item 近似誤差は$p$が増加するにつれて$p = 4$でゼロまで減少
        \item 統計的誤差は$p = 4$後に増加傾向
        \item 一般化リスクは$p = 4$付近で最小値
    \end{itemize}

    統計的誤差は推定値$\hat{\beta}$に依存し，これは訓練セット$\tau$に依存することに注意．統計的誤差のより良い理解を得るため期待される挙動，すなわち多くの訓練セットでの平均的な挙動を考慮する必要がある．
\end{frame}

\subsection{バイアス-分散分解}
\begin{frame}
    \frametitle{バイアス-分散分解の導入}
    もう一度2乗誤差損失を使用し，一般的な$\mathcal{G}$に対するもう一つの分解（バイアス-分散分解）を，近似誤差や統計的誤差の組み合わせで考えていく

    \begin{align*}
        \ell(g_\tau^{\mathcal{G}}) = \ell^* + \ell(g^\mathcal{G}_\tau)-\ell(g^*)
    \end{align*}

    Theorem 2.1の証明と同様の推論を使用すると：

    \begin{align*}
        \ell(g_\tau^{\mathcal{G}}) & = \mathbb{E}[(g_\tau^{\mathcal{G}}(X) - Y)^2] = \ell^* + \mathbb{E}[(g_\tau^{\mathcal{G}}(X) - g^*(X))^2] \\
                                   & =\ell^*+\mathbb{E}\left[D^2(\boldsymbol{X},\tau)\right]
    \end{align*}

    ここで$D(\boldsymbol{x}, \tau) := g_\tau^{\mathcal{G}}(X) - g^*(X)$とする．
\end{frame}

\begin{frame}
    \frametitle{バイアス-分散分解の展開}
    ランダムな訓練セット$\mathcal{T}$に対するランダム変数$D(\boldsymbol{x}, \mathcal{T})$について期待2乗誤差は

    \[
        \mathbb{E}[(g_\tau^{\mathcal{G}}(x) - g^*(x))^2] = \mathbb{E}\left[D^2(\boldsymbol{x},\tau)\right]={\mathbb{E}\left[D(\boldsymbol{x},\tau)\right]}^2+\mathbb{V}\left[D(\boldsymbol{x},\tau)\right]
    \]

    \[
        = \underbrace{(\mathbb{E}[g_\tau^{\mathcal{G}}(x)] - g^*(x))^2}_{\text{pointwise squared bias}} + \underbrace{\mathbb{V}[g_\tau^{\mathcal{G}}(x)]}_{\text{pointwise variance}} \tag{2.21}
    \]

    学習器$g_\tau^{\mathcal{G}}(x)$をランダムな訓練セットの関数と見ると：
    \begin{itemize}
        \item \alert{pointwise squared bias}項は$g_\tau^{\mathcal{G}}(x)$が$g^*(x)$から平均的にどれだけ離れているかを測定
        \item \alert{pointwise variance}項は$g_\tau^{\mathcal{G}}(x)$の期待値からの偏差を測定
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{期待一般化リスクの分解}
    $X$と$\mathcal{T}$が独立であることに注意．したがって，期待一般化リスク(2.6)は以下のように書ける：

    \begin{align}
        \mathbb{E}[\ell(g_\tau^{\mathcal{G}})]
         & = \ell^* + \underbrace{\mathbb{E}\left[\mathbb{E}[g_\tau^{\mathcal{G}}(X) | X] - g^*(X)\right]^2}_\text{expected squared bias} + \underbrace{\mathbb{E}\left[\mathbb{V}(g_\tau^{\mathcal{G}}(X) | X)\right]}_\text{expected variance} \tag{2.22}
    \end{align}

    これが\alert{バイアス-分散トレードオフ}の基本的な分解である．
\end{frame}

\begin{frame}
    \frametitle{バイアス-分散トレードオフの解釈}
    バイアス-分散分解(2.22)から：

    \begin{itemize}
        \item \alert{Expected squared bias}: 学習器が真の関数から系統的にどれだけ偏っているかを表す
              \begin{itemize}
                  \item 単純なモデル（線形回帰など）は高バイアス
                  \item 複雑なモデルは低バイアス
              \end{itemize}

        \item \alert{Expected variance}: 異なる訓練セットに対する学習器の予測のばらつきを表す
              \begin{itemize}
                  \item 単純なモデルは低分散
                  \item 複雑なモデル（高次多項式など）は高分散
              \end{itemize}
    \end{itemize}

    \vspace{0.5cm}
    \alert{トレードオフ}: モデルの複雑さを増すとバイアスは減るが分散は増える．最適な複雑さは両者のバランスで決まる．
\end{frame}

\end{document}
